{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected vs Convoluted neural networks for MNIST classification\n",
    "### Task 1\n",
    "In this section I create a simple neural network class, SimpleFCNN. This class has two linear layers, which transform the input of (28 x 28) to 784 -> 64 -> 10. Relu is applied to the first layer.\n",
    "\n",
    "I then retrieve and organise the MNIST data into seperate sets, define a method to return accuracy and loss averages and then train an instance of the SimpleFCNN class. The accuracy and loss averages of this trained instance is then outputted in graphs. The instance is trained with a batched training set over 100 epochs. I showcase the overfitting nature of the neural network in the resultant graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qRRv6gwh7eK"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# note: mnist must be in grayscale and (batch_size, 1, 28, 28)\n",
    "# use torchvision.transforms to do this\n",
    "class SimpleFCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 64)\n",
    "        self.fc2 = nn.Linear(64, 10) # Output 10 (one for each digit)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input (assuming input size is 28x28)\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first hidden layer\n",
    "        x = self.fc2(x)  # Output layer (no activation function)\n",
    "        return x\n",
    "\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss() \n",
    "\n",
    "# retrieve and organise data\n",
    "transform_list = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=[0.0], std=[1.0,]) ] )\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_list)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_list)\n",
    "\n",
    "mnist_trainset_small = [ mnist_trainset[i] for i in range(0,4000) ]\n",
    "\n",
    "\n",
    "def getAccuracyAndMeanLoss(trainloader, testloader, network):\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader: \n",
    "            images, labels = data\n",
    "            outputs = network(images)\n",
    "            loss = loss_function (outputs, labels)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    train_loss = total_loss / total\n",
    "\n",
    "\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader: \n",
    "            images, labels = data\n",
    "            outputs = network(images)\n",
    "            _, predicted = torch.max( outputs, dim=1)\n",
    "            loss = loss_function (outputs, labels)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item() \n",
    "            total_loss += loss.item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_loss = total_loss / total\n",
    "\n",
    "    return train_accuracy, test_accuracy, train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader( mnist_trainset_small, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader( mnist_testset, batch_size=32, shuffle=True)\n",
    "\n",
    "nn1 = SimpleFCNN()\n",
    "optimizer = torch.optim.Adam(nn1.parameters(), lr=0.0001)\n",
    "\n",
    "test_accuracy_over_time = []\n",
    "train_accuracy_over_time = []\n",
    "test_loss_over_time = []\n",
    "train_loss_over_time = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    current_loss = 0.0 \n",
    "    n_mini_batches = 0\n",
    "    \n",
    "    for i, mini_batch in enumerate( trainloader, 0 ):\n",
    "        images, labels = mini_batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = nn1(images)\n",
    "        loss = loss_function ( outputs, labels )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_mini_batches += 1 \n",
    "        current_loss += loss.item()        \n",
    "     \n",
    "    print('Epoch %d loss: %.3f' %(epoch+1, current_loss / n_mini_batches ))\n",
    "    train_accuracy, test_accuracy, train_loss, test_loss = getAccuracyAndMeanLoss(trainloader, testloader, nn1)\n",
    "\n",
    "    test_accuracy_over_time.append(test_accuracy)\n",
    "    train_accuracy_over_time.append(train_accuracy)\n",
    "    test_loss_over_time.append(test_loss)\n",
    "    train_loss_over_time.append(train_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(test_accuracy_over_time, label='Test Accuracy')\n",
    "axs[0].plot(train_accuracy_over_time, label='Train Accuracy')\n",
    "\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Accuracy of estimations')\n",
    "axs[0].set_title('Network accuracy against training epoch')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(test_loss_over_time, label='Test Loss')\n",
    "axs[1].plot(train_loss_over_time, label='Train Loss')\n",
    "\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Average Loss on estimations')\n",
    "axs[1].set_title('Network loss against training epoch')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph commentary\n",
    "Both graphs clearly indicate that the neural network has overfitted to the training set. The test set plateaus at approximately 90% accuracy and 0.01 avaerage loss, whereas the training set approaches 100% and 0.00. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 - Discussion\n",
    "For this task, I have chosen to test different hyperparameter values for l2 regularisation. I use np.linspace to generate 25 values between 0.00000001 and 0.01 and use these (with the Adam optimiser with learning rate of 0.0001, same as previous work) to train 25 instances of the SimpleFCNN. The final loss and acccuracy averages for these networks are then plotted and compared. The best performing hyperparameter is then chosen based on these results and used in later work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sehR4_nh7eL"
   },
   "outputs": [],
   "source": [
    "interval_number = 25\n",
    "weight_decay_values = np.linspace(0.00000001, 0.01, num=interval_number)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader( mnist_trainset_small, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader( mnist_testset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_accuracy_values = []\n",
    "train_accuracy_values = []\n",
    "test_loss_values = []\n",
    "train_loss_values = []\n",
    "net_count = 0\n",
    "\n",
    "for weight_decay in weight_decay_values:\n",
    "    net_count += 1\n",
    "    print(\"Training neural network %d / %d\" %(net_count, interval_number))\n",
    "    \n",
    "    nn1 = SimpleFCNN()\n",
    "    optimizer = torch.optim.Adam(nn1.parameters(), lr=0.0001, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        current_loss = 0.0 \n",
    "        n_mini_batches = 0\n",
    "\n",
    "        for i, mini_batch in enumerate( trainloader, 0 ):\n",
    "            images, labels = mini_batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = nn1(images)\n",
    "            loss = loss_function ( outputs, labels )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_mini_batches += 1 \n",
    "            current_loss += loss.item()\n",
    "\n",
    "    train_accuracy, test_accuracy, train_loss, test_loss = getAccuracyAndMeanLoss(trainloader, testloader, nn1)\n",
    "    train_accuracy_values.append(train_accuracy)\n",
    "    test_accuracy_values.append(test_accuracy)\n",
    "    train_loss_values.append(train_loss)\n",
    "    test_loss_values.append(test_loss)\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(test_accuracy_values, label='Test Accuracy')\n",
    "axs[0].plot(train_accuracy_values, label='Train Accuracy')\n",
    "\n",
    "axs[0].set_xticks(range(len(weight_decay_values)), \\\n",
    "                  ['{:.2e}'.format(value) for value in weight_decay_values], \\\n",
    "                  rotation=90)\n",
    "axs[0].set_xlabel('Weight Decay (L2)')\n",
    "axs[0].set_ylabel('Accuracy of estimations')\n",
    "axs[0].set_title('Network accuracy against L2 regularization value')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(test_loss_values, label='Test Loss')\n",
    "axs[1].plot(train_loss_values, label='Train Loss')\n",
    "\n",
    "axs[1].set_xticks(range(len(weight_decay_values)), \\\n",
    "                  ['{:.2e}'.format(value) for value in weight_decay_values], \\\n",
    "                  rotation=90)\n",
    "axs[1].set_xlabel('Weight Decay (L2)')\n",
    "axs[1].set_ylabel('Average Loss on estimations')\n",
    "axs[1].set_title('Network loss against L2 regularization value')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From these results, the regularization value with the best test data accuracy is {:.2e}\"\n",
    "      .format(weight_decay_values[test_accuracy_values.index(max(test_accuracy_values))]))  \n",
    "print(\"Accuracy value is:\", max(test_accuracy_values))\n",
    "print(\"Loss value is:\", test_loss_values[test_accuracy_values.index(max(test_accuracy_values))])\n",
    "\n",
    "first_NN_weight_deacy = weight_decay_values[test_accuracy_values.index(max(test_accuracy_values))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result discussion\n",
    "The optimum l2 regularization hyperparameter value is the value that has the highest accuracy and lowest loss. However, the value 1.00e-08 provides the best accuracy and 1.67e-03 provides the best average loss. In this instance, I have chosen to favour accuracy - accuracy measures the ratio of correct to incorrect predictions, rather than the magnitude of incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 - Discussion\n",
    "Using the l2 regularisation parameter chosen from task 2, I have trained 8 instances of the SimpleFCNN on different training set sizes, 500, 1000, 2000, 4000, 8000, 16000, 32000, 60000. The accuracy and loss averages (for the test set) have been plotted against the training set size. These graphs are in log log configuration to ease comparison (ideal improvemnet rate has a gradient of 1/sqrt(N), which is a straight line in log log graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtvmRB_dh7eM"
   },
   "outputs": [],
   "source": [
    "training_set_sizes = [500, 1000, 2000, 4000, 8000, 16000, 32000, 60000]\n",
    "# use the same testloader as instantiated beforehand.\n",
    "\n",
    "test_NN_accuracy_values = []\n",
    "test_NN_loss_values = []\n",
    "net_count = 0\n",
    "weight_decay = first_NN_weight_deacy\n",
    "\n",
    "for set_size in training_set_sizes:\n",
    "    # define trainset\n",
    "    mnist_trainsubset = [ mnist_trainset[i] for i in range(0,set_size) ]\n",
    "    trainloader = torch.utils.data.DataLoader(mnist_trainsubset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    net_count += 1\n",
    "    print(\"Training neural network %d / %d\" %(net_count, len(training_set_sizes)))\n",
    "    \n",
    "    nn1 = SimpleFCNN()\n",
    "    optimizer = torch.optim.Adam(nn1.parameters(), lr=0.0001, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        current_loss = 0.0 \n",
    "        n_mini_batches = 0\n",
    "\n",
    "        for i, mini_batch in enumerate( trainloader, 0 ):\n",
    "            images, labels = mini_batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = nn1(images)\n",
    "            loss = loss_function ( outputs, labels )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_mini_batches += 1 \n",
    "            current_loss += loss.item()\n",
    "\n",
    "    _, test_accuracy, _, test_loss = getAccuracyAndMeanLoss(trainloader, testloader, nn1)\n",
    "    test_NN_accuracy_values.append(test_accuracy)\n",
    "    test_NN_loss_values.append(test_loss)\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axs[0].loglog(training_set_sizes, test_NN_accuracy_values, label='Test Accuracy')\n",
    "axs[0].set_xlabel('Training size')\n",
    "axs[0].set_ylabel('Accuracy of estimations')\n",
    "axs[0].set_title('Network accuracy against Training size')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].loglog(training_set_sizes, test_NN_loss_values, label='Test Loss')\n",
    "\n",
    "def approx_func(x):\n",
    "    return (1 / np.sqrt(x ** 1.3) + 0.002)\n",
    "\n",
    "x_func = np.linspace(500, 60000, 100)\n",
    "plt.loglog(x_func, approx_func(x_func), linestyle='--', color='red', label='y = 1/(sqrt(x^1.3)) + 0.002')\n",
    "\n",
    "\n",
    "axs[1].set_xlabel('Training size')\n",
    "axs[1].set_ylabel('Average Loss on estimations')\n",
    "\n",
    "axs[1].set_title('Network loss against Training size')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"After some experimentation, the loss curve appears to approximately follows y = 1/(sqrt(x^1.3)) + 0.002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result discussion\n",
    "\n",
    "The accuracy and loss averages plots are close to linear in these log log graphs. The loss plot is compared to a modification of the 1/sqrt(x) graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### task 4 - Discussion\n",
    "In this task I create a new neural network class, CNN. This neural network has a sequence of convolutional layers followed by fully connected layers. 25 instances of this network class are trained with different l2 regularization hyperparameters. The resulting loss and accuracy average results are plotted and the hyperparameter value for regulariszation is chosen. \n",
    "\n",
    "Following this, seperate instances of the neural network class are trained on increasing training set sizes (using the previoulsy set l2 parameter) and the resulting accuracy and loss averages on the test set are plotted on a log log graph. The results from the SimpleFCNN experiment is included as well as a 1/sqrt(x) graph for comparison. The graph shows the CNN plot has a much closer gradient to 1/sqrt(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mpJo67Ph7eO"
   },
   "outputs": [],
   "source": [
    "# convolutional neural network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "interval_number = 25\n",
    "weight_decay_values = np.linspace(0.00000001, 0.01, num=interval_number)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader( mnist_trainset_small, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader( mnist_testset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_accuracy_values = []\n",
    "train_accuracy_values = []\n",
    "test_loss_values = []\n",
    "train_loss_values = []\n",
    "net_count = 0\n",
    "\n",
    "for weight_decay in weight_decay_values:\n",
    "    net_count += 1\n",
    "    print(\"\\nTraining neural network %d / %d\" %(net_count, interval_number))\n",
    "    \n",
    "    nn1 = CNN()\n",
    "    optimizer = torch.optim.Adam(nn1.parameters(), lr=0.0001, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(100): # reduced epoch count as network fits data much quicker\n",
    "        current_loss = 0.0 \n",
    "        n_mini_batches = 0\n",
    "\n",
    "        for i, mini_batch in enumerate( trainloader, 0 ):\n",
    "            images, labels = mini_batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = nn1(images)\n",
    "            loss = loss_function ( outputs, labels )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_mini_batches += 1 \n",
    "            current_loss += loss.item()\n",
    "        \n",
    "        sys.stdout.write('\\rProgress: {:.2f}%'.format((epoch + 1)))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    train_accuracy, test_accuracy, train_loss, test_loss = getAccuracyAndMeanLoss(trainloader, testloader, nn1)\n",
    "    train_accuracy_values.append(train_accuracy)\n",
    "    test_accuracy_values.append(test_accuracy)\n",
    "    train_loss_values.append(train_loss)\n",
    "    test_loss_values.append(test_loss)\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(test_accuracy_values, label='Test Accuracy')\n",
    "axs[0].plot(train_accuracy_values, label='Train Accuracy')\n",
    "\n",
    "axs[0].set_xticks(range(len(weight_decay_values)), \\\n",
    "                  ['{:.2e}'.format(value) for value in weight_decay_values], \\\n",
    "                  rotation=90)\n",
    "axs[0].set_xlabel('Weight Decay (L2)')\n",
    "axs[0].set_ylabel('Accuracy of estimations')\n",
    "axs[0].set_title('Network accuracy against L2 regularization value')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(test_loss_values, label='Test Loss')\n",
    "axs[1].plot(train_loss_values, label='Train Loss')\n",
    "\n",
    "axs[1].set_xticks(range(len(weight_decay_values)), \\\n",
    "                  ['{:.2e}'.format(value) for value in weight_decay_values], \\\n",
    "                  rotation=90)\n",
    "axs[1].set_xlabel('Weight Decay (L2)')\n",
    "axs[1].set_ylabel('Average Loss on estimations')\n",
    "axs[1].set_title('Network loss against L2 regularization value')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From these results, the regularization value with the best test data accuracy is {:.2e}\"\n",
    "      .format(weight_decay_values[test_accuracy_values.index(max(test_accuracy_values))]))  \n",
    "print(\"Accuracy value is:\", max(test_accuracy_values))\n",
    "print(\"Loss value is:\", test_loss_values[test_accuracy_values.index(max(test_accuracy_values))])\n",
    "\n",
    "second_NN_weight_decay = weight_decay_values[test_accuracy_values.index(max(test_accuracy_values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_sizes = [500, 1000, 2000, 4000, 8000, 16000, 32000, 60000]\n",
    "# use the same testloader as instantiated beforehand.\n",
    "\n",
    "test_CNN_accuracy_values = []\n",
    "test_CNN_loss_values = []\n",
    "net_count = 0\n",
    "weight_decay = second_NN_weight_decay\n",
    "\n",
    "for set_size in training_set_sizes:\n",
    "    # define trainset\n",
    "    mnist_trainsubset = [ mnist_trainset[i] for i in range(0,set_size) ]\n",
    "    trainloader = torch.utils.data.DataLoader(mnist_trainsubset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    net_count += 1\n",
    "    print(\"\\nTraining neural network %d / %d\" %(net_count, len(training_set_sizes)))\n",
    "    \n",
    "    nn1 = CNN()\n",
    "    optimizer = torch.optim.Adam(nn1.parameters(), lr=0.0001, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        current_loss = 0.0 \n",
    "        n_mini_batches = 0\n",
    "\n",
    "        for i, mini_batch in enumerate( trainloader, 0 ):\n",
    "            images, labels = mini_batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = nn1(images)\n",
    "            loss = loss_function ( outputs, labels )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_mini_batches += 1 \n",
    "            current_loss += loss.item()\n",
    "        \n",
    "        sys.stdout.write('\\rProgress: {:.2f}%'.format(epoch + 1))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    _, test_accuracy, _, test_loss = getAccuracyAndMeanLoss(trainloader, testloader, nn1)\n",
    "    test_CNN_accuracy_values.append(test_accuracy)\n",
    "    test_CNN_loss_values.append(test_loss)\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axs[0].loglog(training_set_sizes, test_CNN_accuracy_values, label='CNN Test Accuracy')\n",
    "axs[0].loglog(training_set_sizes, test_NN_accuracy_values, label='First NN Test Accuracy')\n",
    "axs[0].set_xlabel('Training size')\n",
    "axs[0].set_ylabel('Accuracy of estimations')\n",
    "axs[0].set_title('Network accuracy against Training size')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].loglog(training_set_sizes, test_CNN_loss_values, label='CNN Test Loss')\n",
    "axs[1].loglog(training_set_sizes, test_NN_loss_values, label='First NN Test Loss')\n",
    "\n",
    "def approx_func_2(x):\n",
    "    return (1 / np.sqrt(x ** 1))\n",
    "\n",
    "x_func = np.linspace(500, 60000, 100)\n",
    "plt.loglog(x_func, approx_func_2(x_func), linestyle='--', color='red', label='y = 1/(sqrt(x))')\n",
    "\n",
    "\n",
    "axs[1].set_xlabel('Training size')\n",
    "axs[1].set_ylabel('Average Loss on estimations')\n",
    "\n",
    "axs[1].set_title('Network loss against Training size')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result discussion\n",
    "The CNN and SimpleFCNN results are plotted on both graphs. In both, the accuracy and loss averages are better in the CNN network. Additonally, the CNN plot for average loss has a smoother gradient than SimpleFCNN, and is a better fit to 1/sqrt(x)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
